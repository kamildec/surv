{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate\n",
    "import scipy.optimize\n",
    "import pandas as pd\n",
    "import sksurv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp1(N=1000, seed=42, rho=1.0):\n",
    "    gen = np.random.default_rng(seed=seed)\n",
    "\n",
    "    def h0(t):\n",
    "        return np.exp(-17.8+6.5*t-11*np.sqrt(t)*np.log(t)\\\n",
    "            +9.5*np.sqrt(t))\n",
    "    \n",
    "    def h(t, x1, x2, x3, x4, x5):\n",
    "        return h0(t) * np.exp((-0.9+0.1*t+0.9*np.log(t))*x1*rho\\\n",
    "            +0.5*x2-0.2*x3+0.1*x4+1e-6*x5)\n",
    "    \n",
    "    x1 = gen.binomial(n=1, p=0.5, size=N)\n",
    "    x2 = gen.binomial(n=1, p=0.5, size=N)\n",
    "    x3 = gen.normal(loc=10, scale=np.sqrt(2), size=N)\n",
    "    x4 = gen.normal(loc=20, scale=2, size=N)\n",
    "    x5 = gen.normal(loc=0, scale=1, size=N)\n",
    "\n",
    "    def S(t, *args):\n",
    "        H = scipy.integrate.quad(h, 0.0, t, args=args)[0]\n",
    "        return np.exp(-H)\n",
    "    \n",
    "    U = gen.uniform(low=0.0, high=1.0, size=N)\n",
    "\n",
    "    def _T(u, *args):\n",
    "        t0, t1 = 1e-16, 1.0\n",
    "        while S(t1, *args) - u > 0.0:\n",
    "            t1 = 2.0 * t1\n",
    "        \n",
    "        sol = scipy.optimize.root_scalar(\n",
    "            f=lambda t: S(t, *args) - u,\n",
    "            bracket=[t0, t1],\n",
    "        )\n",
    "        return sol.root\n",
    "    \n",
    "    T = np.array([\n",
    "        _T(U[i], x1[i], x2[i], x3[i], x4[i], x5[i])\n",
    "        for i in range(N)\n",
    "    ])\n",
    "\n",
    "    C_l = gen.uniform(low=11.0, high=16.0, size=N)\n",
    "    C_r = gen.uniform(low=0.0, high=24.0, size=N)\n",
    "    y = np.min(np.column_stack((T, C_l, C_r)), axis=1)\n",
    "    delta = np.where((C_l > T) & (C_r > T), 1, 0)\n",
    "\n",
    "    return pd.DataFrame.from_dict(dict(\n",
    "        x1=x1, x2=x2, x3=x3, x4=x4, x5=x5,\n",
    "        duration=y, event=delta,\n",
    "    ))\n",
    "\n",
    "exp1_df = exp1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere(gen: np.random.Generator, center, R, size=None):\n",
    "    dim = center.shape[0]\n",
    "    pts = gen.multivariate_normal(\n",
    "        mean=np.zeros(dim),\n",
    "        cov=np.eye(dim),\n",
    "        size=size,\n",
    "    )\n",
    "    pts = pts / np.linalg.norm(pts, axis=-1).reshape(-1, 1)\n",
    "    return center + pts * R\n",
    "\n",
    "def lime_dataset(center, b, lambda_=1e-5, v=2, N=1000, seed=42):\n",
    "    gen = np.random.default_rng(seed=seed)\n",
    "\n",
    "    x = sphere(gen, np.asarray(center), 8.0, size=N)\n",
    "    U = gen.uniform(low=0.0, high=1.0, size=N)\n",
    "    y = (-np.log(U)/(lambda_*np.exp(np.dot(x, b))))**(1/v)\n",
    "    delta = gen.binomial(n=1, p=0.9, size=N)\n",
    "\n",
    "    return pd.DataFrame.from_dict(dict(\n",
    "        x0=x[:,0], x1=x[:,1], x2=x[:,2], x3=x[:,3], x4=x[:,4],\n",
    "        duration=y, event=delta,\n",
    "    ))\n",
    "\n",
    "lime_df0 = lime_dataset(\n",
    "    center=np.zeros(5),\n",
    "    b=np.array([1e-6, 0.1, -0.15, 1e-6, 1e-6]),\n",
    ")\n",
    "\n",
    "lime_df1 = lime_dataset(\n",
    "    center=np.array([4.0, -8.0, 2.0, 4.0, 2.0]),\n",
    "    b=np.array([1e-6, -0.15, 1e-6, 1e-6, -0.1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heart_failure(path=\"./data/heart_failure.csv\"):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.rename(columns={\n",
    "        \"time\": \"duration\",\n",
    "        \"DEATH_EVENT\": \"event\",\n",
    "    })\n",
    "    return df\n",
    "\n",
    "heart_df = heart_failure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.datasets import get_x_y\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = exp1_df\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_X, train_y = get_x_y(train_df, \n",
    "    attr_labels=[\"event\", \"duration\"], pos_label=1)\n",
    "test_X, test_y = get_x_y(test_df, \n",
    "    attr_labels=[\"event\", \"duration\"], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5632143852801784"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "cph = CoxPHSurvivalAnalysis()\n",
    "cph = cph.fit(train_X, train_y)\n",
    "cph.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5849595762475607"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "rsf = RandomSurvivalForest(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    min_samples_split=8,\n",
    "    min_samples_leaf=4,\n",
    "    max_features=3,\n",
    "    max_samples=0.8,\n",
    ")\n",
    "\n",
    "rsf = rsf.fit(train_X, train_y)\n",
    "rsf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.models import DeepHitSingle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "tf = DeepHitSingle.label_transform(cuts=16)\n",
    "\n",
    "train_y_pcx = tf.fit_transform(train_y[\"duration\"], train_y[\"event\"])\n",
    "test_y_pcx = tf.fit_transform(test_y[\"duration\"], test_y[\"event\"])\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(test_X.shape[-1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.Dropout1d(p=0.5),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.Dropout1d(p=0.5),\n",
    "    nn.Linear(32, tf.out_features),\n",
    ")\n",
    "\n",
    "deephit = DeepHitSingle(\n",
    "    net=net,\n",
    "    optimizer=optim.Adam(net.parameters(), lr=1e-3),\n",
    "    duration_index=tf.cuts,\n",
    ")\n",
    "\n",
    "log = deephit.fit(\n",
    "    input=train_X.values.astype(np.float32),\n",
    "    target=train_y_pcx,\n",
    "    batch_size=64,\n",
    "    epochs=32,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitreus/.local/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  assert pd.Series(self.index_surv).is_monotonic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5900453881067774"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "surv = deephit.interpolate(16).predict_surv_df(test_X.values.astype(np.float32))\n",
    "\n",
    "eval_ = EvalSurv(\n",
    "    surv=surv,\n",
    "    durations=test_y_pcx[0],\n",
    "    events=test_y_pcx[1],\n",
    "    censor_surv=\"km\",\n",
    ")\n",
    "\n",
    "eval_.concordance_td(method=\"antolini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitreus/.local/lib/python3.10/site-packages/captum/attr/_core/deep_lift.py:336: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import DeepLiftShap\n",
    "\n",
    "deep_lift = DeepLiftShap(deephit.net)\n",
    "\n",
    "def deep_lift_results(X: pd.DataFrame):\n",
    "    inputs = torch.tensor(X.values)\n",
    "    baselines = inputs.mean(dim=0).broadcast_to(inputs.shape)\n",
    "    \n",
    "    attr_values = []\n",
    "    for idx in range(tf.out_features):\n",
    "        var_attr_values = deep_lift.attribute(inputs, baselines, target=idx)\n",
    "        attr_values.append(var_attr_values)\n",
    "    attr_values = torch.stack(attr_values, dim=2)\n",
    "\n",
    "    return attr_values.detach().numpy()\n",
    "\n",
    "deep_attr = deep_lift_results(test_X.iloc[:10].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from survshap import SurvivalModelExplainer, ModelSurvSHAP\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "\n",
    "class PycoxAdapter:\n",
    "    def __init__(self, model, cuts):\n",
    "        self.model = model\n",
    "        self.cuts = cuts\n",
    "\n",
    "    def predict_survival_function(self, X):\n",
    "        X = np.asarray(X)\n",
    "        values = self.model.predict_surv(X)\n",
    "        return [interp1d(self.cuts, v) for v in values]\n",
    "    \n",
    "    def predict_cumulative_hazard_function(self, X):\n",
    "        X = np.asarray(X)\n",
    "        values = self.model.predict_hazard(X)\n",
    "        return [lambda t: quad(interp1d(self.cuts, v), 0, t) for v in values]\n",
    "\n",
    "def survshap_results(X: pd.DataFrame, y):\n",
    "    exp = SurvivalModelExplainer(\n",
    "        model=PycoxAdapter(deephit, tf.cuts),\n",
    "        data=X,\n",
    "        y=y,\n",
    "    )\n",
    "\n",
    "    shap = ModelSurvSHAP()\n",
    "    shap.fit(exp, timestamps=tf.cuts)\n",
    "    res_df: pd.DataFrame = shap.full_result\n",
    "\n",
    "    g = res_df.groupby(by=\"variable_name\")\n",
    "    skip = [\"variable_str\", \"variable_name\", \"variable_value\", \"B\", \"aggregated_change\", \"index\"]\n",
    "\n",
    "    attr_values = {}\n",
    "    for var in g.groups:\n",
    "        grp: pd.DataFrame = g.get_group(var)\n",
    "        grp = grp.sort_values(by=[\"index\"])\n",
    "        var_attr_values = grp.iloc[:,len(skip):].values\n",
    "        attr_values[var] = var_attr_values\n",
    "    \n",
    "    attr_values = [attr_values[var] for var in X.columns]\n",
    "    attr_values = np.stack(attr_values, axis=1)\n",
    "    return attr_values\n",
    "    \n",
    "\n",
    "sshap_attrs = survshap_results(test_X.iloc[:10].astype(np.float32), test_y[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
